{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-1\n",
    "This code extracts text and metadata from a PDF file using the `unstructured` library.\n",
    "\n",
    "1. **PDF Partitioning**: It uses `partition_pdf` to read and parse the provided PDF file (`\"Physics 9.pdf\"`).\n",
    "2. **Extract Elements**: It extracts the elements from the PDF into a list of objects, each representing a distinct part of the PDF (such as text or images).\n",
    "3. **Chunk Creation**: For each element, a dictionary is created containing:\n",
    "   - `type`: The class name of the element (e.g., title, narrative text).\n",
    "   - `text`: The actual text content of the element.\n",
    "   - `page_number`: The page number where the element is located (if available).\n",
    "4. **Output**: Finally, it prints the first chunk from the list to verify the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.text import partition_text\n",
    "\n",
    "# Use Unstructured to partition the combined text into chunks\n",
    "elements = partition_text(\"Physics 9.pdf\")\n",
    "\n",
    "# Create chunks with page numbers\n",
    "chunks = []\n",
    "current_page_number = 1  # Start with page 1\n",
    "\n",
    "for element in elements:\n",
    "    # Assign the current page number to the chunk\n",
    "    chunk_data = {\n",
    "        'type': element.__class__.__name__,\n",
    "        'text': element.text,\n",
    "        'page_number': current_page_number  # Assign the page number\n",
    "    }\n",
    "    chunks.append(chunk_data)\n",
    "    \n",
    "    # Update the page number if a page break is detected\n",
    "    if \"page_break\" in str(element):  # Check if the element indicates a page break\n",
    "        current_page_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a chunk from the list\n",
    "for chunk in chunks[0:100]:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-2\n",
    "This code generates embeddings for each chunk of text using a pre-trained Sentence-BERT model.\n",
    "\n",
    "1. **Model Loading**: The `SentenceTransformer` class is used to load the pre-trained model `\"all-MiniLM-L6-v2\"`, which is a lightweight model for generating sentence embeddings.\n",
    "2. **Embedding Generation**: For each chunk in the `chunks` list, the text is passed through the model to generate a vector (embedding) that represents the semantic meaning of the text.\n",
    "3. **Storing Embeddings**: The resulting embeddings, which are in the form of a NumPy array, are converted into a list (`tolist()`) to make them suitable for storage or further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained Sentence-BERT model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Number of chunks \",len(chunks))\n",
    "# Generate embeddings for each chunk\n",
    "for chunk in chunks:\n",
    "    chunk[\"embedding\"] = model.encode(chunk[\"text\"]).tolist()  # Convert numpy array to list for storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-3\n",
    "This code connects to an SQLite database and stores chunk data, including text, type, page number, and embeddings, into a table.\n",
    "\n",
    "1. **Database Connection**: The `sqlite3.connect()` method is used to connect to the SQLite database named `\"BOOK_VISION_CHUNKS.db\"`. If the database does not exist, it will be created.\n",
    "2. **Table Creation**: A `chunks` table is created if it does not already exist. The table has columns for `id`, `type`, `text`, `page_number`, and `embedding`. The `embedding` column stores the embedding as a JSON string.\n",
    "3. **Data Insertion**: For each chunk in the `chunks` list, the `INSERT INTO` SQL statement is executed to store the chunk's data (type, text, page number, and embedding) into the table. The `embedding` is stored as a JSON string using `json.dumps()`.\n",
    "4. **Commit and Close**: After inserting the data, the changes are committed to the database, and the connection is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "\n",
    "# Connect to SQLite database (or create one if it doesn't exist)\n",
    "conn = sqlite3.connect(\"BOOK_VISION_CHUNKS.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table to store chunks and embeddings\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS chunks (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    type TEXT,\n",
    "    text TEXT,\n",
    "    page_number INTEGER,\n",
    "    embedding TEXT  -- Store as JSON string\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert chunk data into database\n",
    "for chunk in chunks:\n",
    "    cursor.execute(\"\"\"\n",
    "    INSERT INTO chunks (type, text, page_number, embedding) VALUES (?, ?, ?, ?)\n",
    "    \"\"\", (chunk[\"type\"], chunk[\"text\"], chunk[\"page_number\"], json.dumps(chunk[\"embedding\"])))  # Store embedding as JSON\n",
    "\n",
    "# Commit and close\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database\n",
    "connection = sqlite3.connect('BOOK_VISION_CHUNKS.db')\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Fetch all chunks from the database\n",
    "cursor.execute(\"SELECT id, type, text, page_number FROM chunks\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Print the chunks to the console\n",
    "print(\"\\nðŸ” **All Chunks in Database:**\")\n",
    "for row in rows:\n",
    "    print(f\"ðŸ“„ Chunk ID {row[0]} - Page {row[3]} - Type: {row[1]}\")\n",
    "    print(f\"Text: {row[2]}\")  # Display first 200 characters of text for brevity\n",
    "    print()\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-4\n",
    "This code demonstrates how to use FAISS (Facebook AI Similarity Search) to index and store text embeddings for efficient similarity search.\n",
    "### ðŸ“Œ FAISS Indexing for Text Chunk Retrieval\n",
    "\n",
    "This script loads text chunk embeddings from an SQLite database (`BOOK_VISION_CHUNKS.db`), converts them into a NumPy array, and indexes them using **FAISS** for efficient similarity search. \n",
    "It utilizes **L2 distance** for nearest neighbor search and maps embeddings with their corresponding chunk IDs. \n",
    "The FAISS index is then saved to a file (`BOOK_VISION_FAISS_INDEX.bin`) and reloaded for future queries. \n",
    "Finally, it prints the total indexed embeddings and their shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "embedding_list = []\n",
    "\n",
    "ids = []\n",
    "\n",
    "connection = sqlite3.connect('BOOK_VISION_CHUNKS.db')  # Reopen the connection if it was closed\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT id, embedding FROM chunks\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for row in rows:\n",
    "    chunk_id = row[0]\n",
    "    embedding = json.loads(row[1])  # Convert JSON string back to a list\n",
    "    embedding_list.append(embedding)\n",
    "    ids.append(chunk_id)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "embedding_array = np.array(embedding_list, dtype=np.float32)\n",
    "ids_array = np.array(ids, dtype=np.int64)  # Store actual chunk IDs\n",
    "\n",
    "# Normalize embeddings before indexing\n",
    "faiss.normalize_L2(embedding_array)  # Normalize the embeddings\n",
    "# Initialize FAISS index\n",
    "embedding_dimension = embedding_array.shape[1]\n",
    "index = faiss.IndexFlatIP(embedding_dimension)\n",
    "\n",
    "# Create ID-based FAISS index\n",
    "index_with_ids = faiss.IndexIDMap(index)\n",
    "index_with_ids.add_with_ids(embedding_array, ids_array)  # Store IDs inside FAISS\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index_with_ids, \"BOOK_VISION_FAISS_INDEX.bin\")\n",
    "\n",
    "index = faiss.read_index(\"BOOK_VISION_FAISS_INDEX.bin\")\n",
    "\n",
    "print(index)\n",
    "\n",
    "print(f\"FAISS Index Size: {index.ntotal}\")\n",
    "\n",
    "print(\"Database embedding shape:\", embedding_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP - 5\n",
    "Add functions to filter chunks by word count and type. These functions will help you refine the chunks before and after retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_chunks_by_word_count(chunks, min_words=5):\n",
    "    \"\"\"Filter out chunks that have fewer than `min_words` words.\"\"\"\n",
    "    filtered_chunks = []\n",
    "    for chunk in chunks:\n",
    "        word_count = len(chunk[\"text\"].split())  # Count words in the chunk\n",
    "        if word_count >= min_words:\n",
    "            filtered_chunks.append(chunk)\n",
    "    return filtered_chunks\n",
    "\n",
    "def filter_chunks_by_type(chunks, exclude_types=[\"title\", \"heading\"]):\n",
    "    \"\"\"Filter out chunks of specific types.\"\"\"\n",
    "    filtered_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if chunk[\"type\"] not in exclude_types:\n",
    "            filtered_chunks.append(chunk)\n",
    "    return filtered_chunks\n",
    "\n",
    "def sort_chunks_by_length(chunks):\n",
    "    \"\"\"Sort chunks by word count in descending order.\"\"\"\n",
    "    return sorted(chunks, key=lambda x: len(x[\"text\"].split()), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-5\n",
    "This code defines a function `search_similar_chunks` to retrieve the most similar chunks of text based on a query by utilizing both FAISS and SQLite.\n",
    "\n",
    "1. **Convert Query to Embedding**: The input `query` is encoded into a vector using the pre-trained `SentenceTransformer` model. This transforms the query into the same format as the stored embeddings for comparison.\n",
    "2. **FAISS Search**: FAISS is used to find the `top_k` most similar embeddings from the stored embeddings, based on cosine similarity (or other distance metrics like L2) between the query vector and the indexed embeddings.\n",
    "3. **SQLite Retrieval**: After obtaining the indices of the most similar chunks from FAISS, the code uses these indices to fetch the corresponding chunk data (type, text, and page number) from the SQLite database.\n",
    "4. **Return Results**: The function returns a list of similar chunks, including the type, text, and page number of each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(query, top_k=50, min_words=5, exclude_types=[\"title\", \"heading\"]):\n",
    "    \"\"\"Retrieve top_k most similar chunks from SQLite using FAISS and filter by word count and type.\"\"\"\n",
    "    \n",
    "    # Step 1: Convert query to embedding\n",
    "    query_vector = model.encode(query).reshape(1, -1)\n",
    "    \n",
    "    # Normalize the query embedding\n",
    "    faiss.normalize_L2(query_vector)\n",
    "    \n",
    "    print(\"Query embedding shape:\", query_vector.shape)\n",
    "    \n",
    "    # Step 2: Use FAISS to find nearest embeddings\n",
    "    distances, indices = index.search(query_vector, top_k)  # FAISS returns cosine similarity scores\n",
    "\n",
    "    # Step 3: Retrieve corresponding chunks from SQLite\n",
    "    results = []\n",
    "    for faiss_index in indices[0]:  # FAISS returns indices\n",
    "        if faiss_index == -1:\n",
    "            continue  # Skip invalid index\n",
    "        \n",
    "        # Fetch the actual database row ID corresponding to FAISS index\n",
    "        cursor.execute(\"SELECT id FROM chunks LIMIT 1 OFFSET ?\", (int(faiss_index),))\n",
    "        row_id = cursor.fetchone()\n",
    "        if row_id:\n",
    "            cursor.execute(\"SELECT id, type, text, page_number FROM chunks WHERE id=?\", (row_id[0],))\n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                results.append({\"id\": row[0], \"type\": row[1], \"text\": row[2], \"page_number\": row[3]})\n",
    "\n",
    "    # Step 4: Filter chunks by word count and type\n",
    "    filtered_results = filter_chunks_by_word_count(results, min_words=min_words)\n",
    "    filtered_results = filter_chunks_by_type(filtered_results, exclude_types=exclude_types)\n",
    "    \n",
    "    return filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Search for keywords\n",
    "To further improve retrieval accuracy, consider combining semantic search (using FAISS) with keyword-based search (e.g., BM25). This hybrid approach can help capture both semantic and lexical relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Create a BM25 index for keyword-based search\n",
    "corpus = [chunk[\"text\"] for chunk in chunks]\n",
    "bm25 = BM25Okapi(corpus)\n",
    "\n",
    "# Perform hybrid search\n",
    "def hybrid_search(query, top_k=100, min_words=5, exclude_types=[]):\n",
    "    \"\"\"Retrieve top_k most similar chunks using hybrid search and filter by word count and type.\"\"\"\n",
    "    \n",
    "    # Step 1: Semantic search with FAISS\n",
    "    faiss_results = search_similar_chunks(query, top_k, min_words=min_words, exclude_types=exclude_types)\n",
    "    \n",
    "    # Step 2: Keyword search with BM25\n",
    "    tokenized_query = query.split(\" \")\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    bm25_indices = np.argsort(bm25_scores)[-top_k:][::-1]  # Get top_k indices\n",
    "    \n",
    "    # Step 3: Fetch BM25 results with the same structure as FAISS results\n",
    "    bm25_results = []\n",
    "    for idx in bm25_indices:\n",
    "        if idx < len(chunks):  # Ensure the index is within bounds\n",
    "            chunk = chunks[idx]\n",
    "            bm25_results.append({\n",
    "                \"id\": idx,  # Use the index as the ID (or fetch the actual ID from the database if needed)\n",
    "                \"type\": chunk[\"type\"],\n",
    "                \"text\": chunk[\"text\"],\n",
    "                \"page_number\": chunk[\"page_number\"]\n",
    "            })\n",
    "    \n",
    "    # Step 4: Combine results\n",
    "    combined_results = faiss_results + bm25_results\n",
    "    \n",
    "    # Step 5: Remove duplicates (if any)\n",
    "    unique_results = []\n",
    "    seen_ids = set()\n",
    "    for result in combined_results:\n",
    "        if result[\"id\"] not in seen_ids:\n",
    "            unique_results.append(result)\n",
    "            seen_ids.add(result[\"id\"])\n",
    "    \n",
    "    # Step 6: Filter chunks by word count and type\n",
    "    filtered_results = filter_chunks_by_word_count(unique_results, min_words=min_words)\n",
    "    filtered_results = filter_chunks_by_type(filtered_results, exclude_types=exclude_types)\n",
    "    \n",
    "    # Step 7: Sort chunks by length and return top_k\n",
    "    sorted_results = sort_chunks_by_length(filtered_results)\n",
    "    return sorted_results[:top_k]  # Return exactly top_k chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('BOOK_VISION_CHUNKS.db')  # Reopen the connection if it was closed\n",
    "cursor = connection.cursor()\n",
    "query = \"force\"\n",
    "\n",
    "similar_chunks = hybrid_search(query)\n",
    "\n",
    "print(\"\\nðŸ” **Top Relevant Chunks:**\")\n",
    "print(len(similar_chunks))\n",
    "for chunk in similar_chunks:\n",
    "    print(f\"ðŸ“„ ID {chunk['id']} - Page {chunk['page_number']} - {chunk['type']}: {chunk['text']}\")\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP_WORK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
