{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sqlite3\n",
    "import json\n",
    "import sqlite3\n",
    "import faiss\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import sounddevice as sd\n",
    "from transformers import pipeline\n",
    "import openai\n",
    "import my_secrets\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-1\n",
    "This code extracts text and metadata from a PDF file using the `unstructured` library.\n",
    "\n",
    "1. **PDF Partitioning**: It uses `partition_pdf` to read and parse the provided PDF file (`\"Physics 9.pdf\"`).\n",
    "2. **Extract Elements**: It extracts the elements from the PDF into a list of objects, each representing a distinct part of the PDF (such as text or images).\n",
    "3. **Chunk Creation**: For each element, a dictionary is created containing:\n",
    "   - `type`: The class name of the element (e.g., title, narrative text).\n",
    "   - `text`: The actual text content of the element.\n",
    "   - `page_number`: The page number where the element is located (if available).\n",
    "4. **Output**: Finally, it prints the first chunk from the list to verify the extracted data.\n",
    "\n",
    "\n",
    "# STEP-2\n",
    "This code generates embeddings for each chunk of text using a pre-trained Sentence-BERT model.\n",
    "\n",
    "1. **Model Loading**: The `SentenceTransformer` class is used to load the pre-trained model `\"all-MiniLM-L6-v2\"`, which is a lightweight model for generating sentence embeddings.\n",
    "2. **Embedding Generation**: For each chunk in the `chunks` list, the text is passed through the model to generate a vector (embedding) that represents the semantic meaning of the text.\n",
    "3. **Storing Embeddings**: The resulting embeddings, which are in the form of a NumPy array, are converted into a list (`tolist()`) to make them suitable for storage or further processing.\n",
    "\n",
    "\n",
    "# STEP-3\n",
    "This code connects to an SQLite database and stores chunk data, including text, type, page number, and embeddings, into a table.\n",
    "\n",
    "1. **Database Connection**: The `sqlite3.connect()` method is used to connect to the SQLite database named `\"BOOK_VISION_CHUNKS.db\"`. If the database does not exist, it will be created.\n",
    "2. **Table Creation**: A `chunks` table is created if it does not already exist. The table has columns for `id`, `type`, `text`, `page_number`, and `embedding`. The `embedding` column stores the embedding as a JSON string.\n",
    "3. **Data Insertion**: For each chunk in the `chunks` list, the `INSERT INTO` SQL statement is executed to store the chunk's data (type, text, page number, and embedding) into the table. The `embedding` is stored as a JSON string using `json.dumps()`.\n",
    "4. **Commit and Close**: After inserting the data, the changes are committed to the database, and the connection is closed.\n",
    "\n",
    "\n",
    "\n",
    "# STEP-4\n",
    "This code demonstrates how to use FAISS (Facebook AI Similarity Search) to index and store text embeddings for efficient similarity search.\n",
    "### ðŸ“Œ FAISS Indexing for Text Chunk Retrieval\n",
    "\n",
    "This script loads text chunk embeddings from an SQLite database (`BOOK_VISION_CHUNKS.db`), converts them into a NumPy array, and indexes them using **FAISS** for efficient similarity search. \n",
    "It utilizes **L2 distance** for nearest neighbor search and maps embeddings with their corresponding chunk IDs. \n",
    "The FAISS index is then saved to a file (`BOOK_VISION_FAISS_INDEX.bin`) and reloaded for future queries. \n",
    "Finally, it prints the total indexed embeddings and their shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = partition_pdf(\"BOOK.pdf\", strategy=\"hi_res\")\n",
    "\n",
    "# Create chunks with page numbers\n",
    "chunks = []\n",
    "\n",
    "for element in elements:\n",
    "    # Assign the current page number to the chunk\n",
    "    chunk_data = {\n",
    "        'type': element.__class__.__name__,\n",
    "        'text': element.text,\n",
    "        'page_number': element.metadata.to_dict().get(\"page_number\", \"Unknown\")  # Assign the page number\n",
    "    }\n",
    "    chunks.append(chunk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of chunks : \", len(chunks))\n",
    "for chunk in chunks[0:100]:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Number of chunks \",len(chunks))\n",
    "# Generate embeddings for each chunk\n",
    "for chunk in chunks:\n",
    "    chunk[\"embedding\"] = model.encode(chunk[\"text\"]).tolist()  # Convert numpy array to list for storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite database (or create one if it doesn't exist)\n",
    "conn = sqlite3.connect(\"BOOK_VISION_CHUNKS.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table to store chunks and embeddings\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS chunks (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    type TEXT,\n",
    "    text TEXT,\n",
    "    page_number INTEGER,\n",
    "    embedding TEXT  -- Store as JSON string\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert chunk data into database\n",
    "for chunk in chunks:\n",
    "    cursor.execute(\"\"\"\n",
    "    INSERT INTO chunks (type, text, page_number, embedding) VALUES (?, ?, ?, ?)\n",
    "    \"\"\", (chunk[\"type\"], chunk[\"text\"], chunk[\"page_number\"], json.dumps(chunk[\"embedding\"])))  # Store embedding as JSON\n",
    "\n",
    "# Commit and close\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = []\n",
    "\n",
    "ids = []\n",
    "\n",
    "connection = sqlite3.connect('BOOK_VISION_CHUNKS.db')  # Reopen the connection if it was closed\n",
    "cursor = connection.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT id, embedding FROM chunks\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "for row in rows:\n",
    "    chunk_id = row[0]\n",
    "    embedding = json.loads(row[1])  # Convert JSON string back to a list\n",
    "    embedding_list.append(embedding)\n",
    "    ids.append(chunk_id)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "embedding_array = np.array(embedding_list, dtype=np.float32)\n",
    "ids_array = np.array(ids, dtype=np.int64)  # Store actual chunk IDs\n",
    "\n",
    "# Normalize embeddings before indexing\n",
    "faiss.normalize_L2(embedding_array)  # Normalize the embeddings\n",
    "# Initialize FAISS index\n",
    "embedding_dimension = embedding_array.shape[1]\n",
    "index = faiss.IndexFlatIP(embedding_dimension)\n",
    "\n",
    "# Create ID-based FAISS index\n",
    "index_with_ids = faiss.IndexIDMap(index)\n",
    "index_with_ids.add_with_ids(embedding_array, ids_array)  # Store IDs inside FAISS\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index_with_ids, \"BOOK_VISION_FAISS_INDEX.bin\")\n",
    "\n",
    "index = faiss.read_index(\"BOOK_VISION_FAISS_INDEX.bin\")\n",
    "\n",
    "print(index)\n",
    "\n",
    "print(f\"FAISS Index Size: {index.ntotal}\")\n",
    "\n",
    "print(\"Database embedding shape:\", embedding_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_chunks_by_word_count(chunks, min_words=5):\n",
    "    \"\"\"Filter out chunks that have fewer than `min_words` words.\"\"\"\n",
    "    filtered_chunks = []\n",
    "    for chunk in chunks:\n",
    "        word_count = len(chunk[\"text\"].split())  # Count words in the chunk\n",
    "        if word_count >= min_words:\n",
    "            filtered_chunks.append(chunk)\n",
    "    return filtered_chunks\n",
    "\n",
    "def filter_chunks_by_type(chunks, exclude_types=[\"title\", \"heading\"]):\n",
    "    \"\"\"Filter out chunks of specific types.\"\"\"\n",
    "    filtered_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if chunk[\"type\"] not in exclude_types:\n",
    "            filtered_chunks.append(chunk)\n",
    "    return filtered_chunks\n",
    "\n",
    "def sort_chunks_by_length(chunks):\n",
    "    \"\"\"Sort chunks by word count in descending order.\"\"\"\n",
    "    return sorted(chunks, key=lambda x: len(x[\"text\"].split()), reverse=True)\n",
    "\n",
    "\n",
    "def search_similar_chunks(query, top_k=50, min_words=5, exclude_types=[\"title\", \"heading\"]):\n",
    "    \"\"\"Retrieve top_k most similar chunks from SQLite using FAISS and filter by word count and type.\"\"\"\n",
    "    \n",
    "    # Step 1: Convert query to embedding\n",
    "    query_vector = model.encode(query).reshape(1, -1)\n",
    "    \n",
    "    # Normalize the query embedding\n",
    "    faiss.normalize_L2(query_vector)\n",
    "    \n",
    "    print(\"Query embedding shape:\", query_vector.shape)\n",
    "    \n",
    "    # Step 2: Use FAISS to find nearest embeddings\n",
    "    distances, indices = index.search(query_vector, top_k)  # FAISS returns cosine similarity scores\n",
    "\n",
    "    # Step 3: Retrieve corresponding chunks from SQLite\n",
    "    results = []\n",
    "    for faiss_index in indices[0]:  # FAISS returns indices\n",
    "        if faiss_index == -1:\n",
    "            continue  # Skip invalid index\n",
    "        \n",
    "        # Fetch the actual database row ID corresponding to FAISS index\n",
    "        cursor.execute(\"SELECT id FROM chunks LIMIT 1 OFFSET ?\", (int(faiss_index),))\n",
    "        row_id = cursor.fetchone()\n",
    "        if row_id:\n",
    "            cursor.execute(\"SELECT id, type, text, page_number FROM chunks WHERE id=?\", (row_id[0],))\n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                results.append({\"id\": row[0], \"type\": row[1], \"text\": row[2], \"page_number\": row[3]})\n",
    "\n",
    "    # Step 4: Filter chunks by word count and type\n",
    "    filtered_results = filter_chunks_by_word_count(results, min_words=min_words)\n",
    "    filtered_results = filter_chunks_by_type(filtered_results, exclude_types=exclude_types)\n",
    "    \n",
    "    return filtered_results\n",
    "\n",
    "\n",
    "### Hybrid Search\n",
    "# Perform hybrid search\n",
    "def hybrid_search(query, top_k=20, min_words=5, exclude_types=[]):\n",
    "    \"\"\"Retrieve top_k most similar chunks using hybrid search and filter by word count and type.\"\"\"\n",
    "    \n",
    "    # Create a BM25 index for keyword-based search\n",
    "    corpus = [chunk[\"text\"] for chunk in chunks]\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "\n",
    "    # Step 1: Semantic search with FAISS\n",
    "    faiss_results = search_similar_chunks(query, top_k, min_words=min_words, exclude_types=exclude_types)\n",
    "    \n",
    "    # Step 2: Keyword search with BM25\n",
    "    tokenized_query = query.split(\" \")\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    bm25_indices = np.argsort(bm25_scores)[-top_k:][::-1]  # Get top_k indices\n",
    "    \n",
    "    # Step 3: Fetch BM25 results with the same structure as FAISS results\n",
    "    bm25_results = []\n",
    "    for idx in bm25_indices:\n",
    "        if idx < len(chunks):  # Ensure the index is within bounds\n",
    "            chunk = chunks[idx]\n",
    "            bm25_results.append({\n",
    "                \"id\": idx,  # Use the index as the ID (or fetch the actual ID from the database if needed)\n",
    "                \"type\": chunk[\"type\"],\n",
    "                \"text\": chunk[\"text\"],\n",
    "                \"page_number\": chunk[\"page_number\"]\n",
    "            })\n",
    "    \n",
    "    # Step 4: Combine results\n",
    "    combined_results = faiss_results + bm25_results\n",
    "    \n",
    "    # Step 5: Remove duplicates (if any)\n",
    "    unique_results = []\n",
    "    seen_ids = set()\n",
    "    for result in combined_results:\n",
    "        if result[\"id\"] not in seen_ids:\n",
    "            unique_results.append(result)\n",
    "            seen_ids.add(result[\"id\"])\n",
    "    \n",
    "    # Step 6: Filter chunks by word count and type\n",
    "    filtered_results = filter_chunks_by_word_count(unique_results, min_words=min_words)\n",
    "    filtered_results = filter_chunks_by_type(filtered_results, exclude_types=exclude_types)\n",
    "    \n",
    "    # Step 7: Sort chunks by length and return top_k\n",
    "    sorted_results = sort_chunks_by_length(filtered_results)\n",
    "    return sorted_results[:top_k]  # Return exactly top_k chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(duration, sample_rate):\n",
    "    print(\"Recording... Speak now!\")\n",
    "    audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')\n",
    "    sd.wait()\n",
    "    print(\"Recording complete.\")\n",
    "    return np.squeeze(audio)\n",
    "\n",
    "def get_Voice_Input():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    whisper_pipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\", device=device)\n",
    "    SAMPLE_RATE = 16000 \n",
    "    DURATION = 8 \n",
    "    audio_data = record_audio(DURATION, SAMPLE_RATE)\n",
    "    transcription = whisper_pipeline({\"sampling_rate\": SAMPLE_RATE, \"raw\": audio_data}, generate_kwargs={\"language\": \"en\"})['text']\n",
    "    return transcription\n",
    "\n",
    "choice = input(\"Choose input method - Text (T) or Voice (V): \").strip().lower()\n",
    "if choice == 't':\n",
    "    user_input = input(\"Enter your text: \")\n",
    "elif choice == 'v':\n",
    "    user_input = get_Voice_Input()\n",
    "else:\n",
    "    print(\"Invalid choice! Defaulting to text input.\")\n",
    "    user_input = input(\"Enter your text: \")\n",
    "\n",
    "print(\"Your input:\", user_input)\n",
    "\n",
    "connection = sqlite3.connect('BOOK_VISION_CHUNKS.db')  # Reopen the connection if it was closed\n",
    "cursor = connection.cursor()\n",
    "\n",
    "similar_chunks = hybrid_search(user_input)\n",
    "\n",
    "print(\"\\nðŸ” **Top Relevant Chunks:**\")\n",
    "print(len(similar_chunks))\n",
    "for chunk in similar_chunks:\n",
    "    print(f\"ðŸ“„ ID {chunk['id']} - Page {chunk['page_number']} - {chunk['type']}: {chunk['text']}\")\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_openai_key():\n",
    "    return my_secrets.OPEN_AI_SECRET_KEY\n",
    "\n",
    "def call_openai_chat(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    api_key = load_openai_key()\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are an expert in physics.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=4000,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "def save_to_file(filename, content):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "def clean_latex_code(latex_content):\n",
    "    try:\n",
    "        openai.api_key = load_openai_key()\n",
    "        prompt = (\n",
    "            \"You are an expert in LaTeX. Your task is to clean and fix the given LaTeX code, ensuring it compiles without any errors or warnings.\\n\\n\"\n",
    "            \"### Instructions:\\n\"\n",
    "            \"- Fix any syntax issues while **preserving the original structure and content**.\\n\"\n",
    "            \"- Ensure all required LaTeX packages and dependencies are included.\\n\"\n",
    "            \"- Remove any unnecessary whitespace, extra newlines, or redundant commands.\\n\"\n",
    "            \"- Do **not** add, remove, or alter content unless necessary for compilation.\\n\"\n",
    "            \"- Ensure that Beamer presentations have correctly structured frames and do not include unwanted blank pages.\\n\"\n",
    "            \"- Provide **only the cleaned and corrected LaTeX code**, with no explanations or comments.\\n\\n\"\n",
    "            \"### LaTeX Code to Clean and Fix:\\n\"\n",
    "            f\"{latex_content}\"\n",
    "        )\n",
    "\n",
    "        response = call_openai_chat(prompt)\n",
    "        response = response.replace(\"\\maketitle\", \"\")  # Remove unwanted title commands\n",
    "        response = response.replace(\"\\clearpage\", \"\").replace(\"\\\\newpage\", \"\")  # Remove page breaks\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during LaTeX cleaning: {e}\"\n",
    "\n",
    "\n",
    "def generate_pdf_from_latex(latex_code, output_filename):\n",
    "    temp_tex_file = \"temp_presentation.tex\"\n",
    "    save_to_file(temp_tex_file, latex_code)\n",
    "\n",
    "    try:\n",
    "        # Run pdflatex twice for cross-referencing\n",
    "        result = subprocess.run(\n",
    "            [\"pdflatex\", \"-interaction=nonstopmode\", temp_tex_file],\n",
    "            check=True,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        print(\"âœ… PDFLaTeX Output:\\n\", result.stdout)\n",
    "\n",
    "        # Check if the PDF was created\n",
    "        if os.path.exists(\"temp_presentation.pdf\"):\n",
    "            os.rename(\"temp_presentation.pdf\", output_filename)\n",
    "            print(f\"âœ… PDF successfully generated and saved as {output_filename}.\")\n",
    "        else:\n",
    "            print(\"âš ï¸ PDF was not generated. Checking LaTeX logs.\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"ðŸš¨ LaTeX Compilation Error:\")\n",
    "        print(\"STDOUT:\\n\", e.stdout)  # Print LaTeX standard output\n",
    "        print(\"STDERR:\\n\", e.stderr)  # Print LaTeX error messages\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ General error during PDF generation: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Cleanup LaTeX temporary files\n",
    "        for ext in [\"aux\", \"log\", \"out\", \"tex\"]:\n",
    "            if os.path.exists(f\"temp_presentation.{ext}\"):\n",
    "                os.remove(f\"temp_presentation.{ext}\")\n",
    "\n",
    "\n",
    "\n",
    "refinement_prompt = (\n",
    "    \"You are a highly knowledgeable physics expert. Your task is to generate a clear, detailed, and well-structured response to my question, ensuring a strong conceptual understanding and a professional, high-quality impression.\\n\\n\"\n",
    "    f\"### My Question:\\n{user_input}\\n\\n\"\n",
    "    \"### Instructions:\\n\"\n",
    "    \"- Carefully analyze the provided reference text and use only **the most relevant parts** that closely relate to the question.\\n\"\n",
    "    \"- **Ignore any chunks** that are unrelated or do not contribute meaningfully to the response.\\n\"\n",
    "    \"- Craft a well-structured response that explains the core concept in **simple, precise, and engaging language**.\\n\"\n",
    "    \"- Use **real-world examples** to illustrate the topic effectively.\\n\"\n",
    "    \"- Identify and describe any **types, categories, or variations** relevant to the concept.\\n\"\n",
    "    \"- Incorporate **relevant formulas and equations**, ensuring each variable is clearly defined.\\n\"\n",
    "    \"- Provide **step-by-step derivations** where necessary to enhance clarity.\\n\"\n",
    "    \"- Include **key insights, interesting facts, or historical context** to make the explanation more engaging.\\n\"\n",
    "    \"- Ensure the response is **comprehensive, professional, and insightful** to leave a strong impression.\\n\"\n",
    "    \"- Conclude with a set of **engaging follow-up questions** based on the explanation, ensuring that each question aligns with a concept already covered in the text.\\n\\n\"\n",
    "    f\"### Reference Text:\\n{similar_chunks}\\n\\n\"\n",
    "    \"### Final Output:\\n\"\n",
    "    \"1. A **refined, structured response** incorporating deep explanations, examples, formulas, and derivations.\\n\"\n",
    "    \"2. A set of **relevant, thought-provoking tidbit questions** for the student to test their understanding, ensuring each question aligns with a concept already explained in the response.\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "refined_answer = call_openai_chat(refinement_prompt)\n",
    "print(\"\\nGenerated Answer:\")\n",
    "print(refined_answer)\n",
    "\n",
    "output_filename = \"text_response.txt\"\n",
    "save_to_file(output_filename, refined_answer)\n",
    "\n",
    "print(f\"\\nRefined answer saved to {output_filename}.\")\n",
    "# beamer presentation\n",
    "slide_prompt = (\n",
    "    \"You are an expert in LaTeX and Beamer. Your task is to generate a complete, error-free Beamer presentation based on the given text.\\n\\n\"\n",
    "    \"### Instructions:\\n\"\n",
    "    \"- Ensure the **LaTeX code is fully compilable** with no errors.\\n\"\n",
    "    \"- Always include the necessary **\\\\documentclass{beamer}** and **\\\\begin{document}** tags.\\n\"\n",
    "    \"- Structure the presentation into **logically divided frames** for clarity.\\n\"\n",
    "    \"- Use **visual aids**, such as diagrams, equations, or bullet points, where relevant.\\n\"\n",
    "    \"- Highlight key points using **bold text, colors, or overlays** where appropriate.\\n\"\n",
    "    \"- Include **proper slide titles** for each frame to maintain structure.\\n\"\n",
    "    \"- Ensure mathematical expressions are correctly formatted using **LaTeX math mode**.\\n\"\n",
    "    \"- Use relevant **blocks (e.g., theorem, definition, example)** where necessary.\\n\\n\"\n",
    "    \"### Content to Convert into Slides:\\n\"\n",
    "    f\"{refined_answer}\\n\\n\"\n",
    "    \"Generate a complete and structured Beamer LaTeX code that is ready for direct compilation.\"\n",
    ")\n",
    "\n",
    "slide_content = call_openai_chat(slide_prompt)\n",
    "\n",
    "slide_filename = \"presentation.tex\"\n",
    "save_to_file(slide_filename, slide_content)\n",
    "print(f\"\\nPresentation slides saved to {slide_filename}.\")\n",
    "cleaned_slide_content = clean_latex_code(slide_content)\n",
    "\n",
    "cleaned_slide_filename = \"cleaned_presentation.tex\"\n",
    "save_to_file(cleaned_slide_filename, cleaned_slide_content)\n",
    "print(f\"\\nCleaned slide content saved to {cleaned_slide_filename}.\")\n",
    "\n",
    "# Step 9: Generate a PDF from the cleaned LaTeX code\n",
    "pdf_filename = \"MyResponse.pdf\"\n",
    "generate_pdf_from_latex(cleaned_slide_content, pdf_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP_WORK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
